{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6321d045-ba35-4316-b9c5-907bb1c20dd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a1730c6a-b4c2-4889-94f0-a4144817d472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "single_match=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9600c8e9-f8bd-4c05-bde5-355db9db6dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a5f1887c-dc9e-41d1-925b-42294e2ced97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if single_match:\n",
    "    from utils.ex_aspire_consent import AspireConSent, prepare_abstracts\n",
    "else:\n",
    "    from utils.ex_aspire_consent_multimatch import AspireConSent, AllPairMaskedWasserstein\n",
    "    from utils.ex_aspire_consent_multimatch import prepare_abstracts\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0727cef-89d6-4b56-9668-76b91a8d1e5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ead63-9ef6-4030-974c-27ab2ce79520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import utils.envsetup\n",
    "from data.pmc_iterable import PMCIterable\n",
    "from data.topic_iterable import TopicIterable\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.doc_abstract_to_sentence_list_transform import DocAbstractToSentenceListTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7b6107-064a-468b-8148-fb18854bc3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pmc_ids = pickle.load(open('data/data_old_format/pmc_ids.pkl', 'rb'))\n",
    "\n",
    "topics_trainloader = DataLoader(TopicIterable(format='aspire', train=True, topic_file_name='data/data_old_format/topics.pkl',\n",
    "                                             transform=DocAbstractToSentenceListTransform()), batch_size=1,\n",
    "                               collate_fn=lambda x: x)\n",
    "\n",
    "topics_trainloader = sorted([x for x in topics_dataloader], key=lambda x: int(x[0]['ID']))\n",
    "\n",
    "topics_testloader = DataLoader(TopicIterable(format='aspire', test=True, topic_file_name='data/data_old_format/topics.pkl',\n",
    "                                             transform=DocAbstractToSentenceListTransform()), batch_size=1,\n",
    "                               collate_fn=lambda x: x)\n",
    "\n",
    "topics_testloader = sorted([x for x in topics_testloader], key=lambda x: int(x[0]['ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a5b7104-021a-414c-864d-6dea9674bc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/data_old_format/labels_dict.pkl\", 'rb') as file:\n",
    "    labelled_article_ids_pickle = pickle.load(file)\n",
    "\n",
    "labelled_article_ids = {}\n",
    "for key in labelled_article_ids_pickle.keys():\n",
    "    positive = [k for k,v in labelled_article_ids_pickle[key].items() if v in ['1','2']]\n",
    "    negative = [k for k,v in labelled_article_ids_pickle[key].items() if v == '0']\n",
    "    labelled_article_ids[key] = {\"POSITIVE\": positive, \"NEGATIVE\": negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "02c874bd-fd29-4a4d-b6ab-f7fca3339661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loadtime: 61.04307842254639\n",
      "13199\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "train_triplets = []\n",
    "negs_per_pos = 5\n",
    "for topic in topics_trainloader:\n",
    "    topic = topic[0]\n",
    "    topic_id = topic['ID']\n",
    "    articles_positive_loader = DataLoader(PMCIterable(labeled_ids_or_filename=labelled_article_ids[topic_id]['POSITIVE'], format='aspire',\n",
    "                                                      transform=DocAbstractToSentenceListTransform()), batch_size=1, collate_fn=lambda x: x)\n",
    "    articles_negative_loader = DataLoader(PMCIterable(labeled_ids_or_filename=labelled_article_ids[topic_id]['NEGATIVE'], format='aspire',\n",
    "                                                      transform=DocAbstractToSentenceListTransform()), batch_size=1, collate_fn=lambda x: x)\n",
    "    iterator = iter(articles_negative_loader)\n",
    "    for pos in articles_positive_loader:\n",
    "        pos = pos[0]\n",
    "        for _ in range(negs_per_pos):\n",
    "            try:\n",
    "                neg = next(iterator)[0]\n",
    "                train_triplets.append(\n",
    "                    (topic, pos, neg)\n",
    "                )\n",
    "            except StopIteration as e:\n",
    "                break\n",
    "print(\"Dataset Loadtime: {}\".format(time.time() - start))\n",
    "print(len(train_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1261be4f-960a-410d-8b92-f30d889a917e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loadtime: 19.802971839904785\n",
      "2779\n"
     ]
    }
   ],
   "source": [
    "transform_to_sentence_list = DocAbstractToSentenceListTransform()\n",
    "import time\n",
    "start = time.time()\n",
    "test_triplets = []\n",
    "negs_per_pos = 5\n",
    "for topic in topics_testloader:\n",
    "    topic = topic[0]\n",
    "    topic_id = topic['ID']\n",
    "    articles_positive_loader = DataLoader(PMCIterable(labeled_ids_or_filename=labelled_article_ids[topic_id]['POSITIVE'], format='aspire',\n",
    "                                                      transform=DocAbstractToSentenceListTransform()), batch_size=1, collate_fn=lambda x: x)\n",
    "    articles_negative_loader = DataLoader(PMCIterable(labeled_ids_or_filename=labelled_article_ids[topic_id]['NEGATIVE'], format='aspire',\n",
    "                                                      transform=DocAbstractToSentenceListTransform()), batch_size=1, collate_fn=lambda x: x)\n",
    "    iterator = iter(articles_negative_loader)\n",
    "    for pos in articles_positive_loader:\n",
    "        pos = pos[0]\n",
    "        for _ in range(negs_per_pos):\n",
    "            try:\n",
    "                neg = next(iterator)[0]\n",
    "                test_triplets.append(\n",
    "                    (topic, pos, neg)\n",
    "                )\n",
    "            except StopIteration as e:\n",
    "                break\n",
    "print(\"Dataset Loadtime: {}\".format(time.time() - start))\n",
    "print(len(test_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8b8100a4-3ddb-49cd-9f01-8ceb5862aef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/data_new_format/triplets/train_triplets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(train_triplets, file)\n",
    "    \n",
    "import pickle\n",
    "with open(\"data/data_new_format/triplets/test_triplets.pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_triplets, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d8e75-ad86-40a9-8e1e-ec440c2ec7f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Triplets to top sentence Triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ef23f-2b0f-4230-806c-d3278d682970",
   "metadata": {},
   "source": [
    "Performed In Parallel (Long Computation) In \"data/data_new_format/triplets/triplets_to_sentence_only_triplets.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3b96f729-53ea-45db-a82c-87ae8c0bd3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# aspire_sent = AutoModel.from_pretrained('allenai/aspire-sentence-embedder')\n",
    "# aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-sentence-embedder')\n",
    "\n",
    "# def apply_sent_bert(sents):\n",
    "#     inputs = aspire_tok(sents, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "#     result = aspire_sent(**inputs)\n",
    "\n",
    "#     clsrep = result.last_hidden_state[:,0,:]\n",
    "    \n",
    "#     return clsrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7fc57-4a0a-49d2-a550-95854df8b273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_triplets_sentence_only = []\n",
    "# for triplet in tqdm.tqdm(test_triplets):\n",
    "#     # generate embeddings\n",
    "#     triplet = train_triplets[0]\n",
    "#     topic, pos, neg = triplet[0]['ABSTRACT'], triplet[1]['ABSTRACT'], triplet[2]['ABSTRACT']\n",
    "#     co_citation_context = triplet[0][\"CO-CITATION-CONTEXT\"]\n",
    "#     topic_embed, pos_embed, neg_embed, context_embed = apply_sent_bert(topic), apply_sent_bert(pos), apply_sent_bert(pos), apply_sent_bert(co_citation_context)\n",
    "    \n",
    "#     # create sentence-only triplet\n",
    "#     triplet_sentence_only = []\n",
    "#     for sentences, embedding in zip([topic, pos, neg], [topic_embed, pos_embed, neg_embed]):\n",
    "#         distance_topic_context = torch.squeeze(torch.cdist(embedding, context_embed, p=2.0), 0)\n",
    "#         argmax = torch.argmin(distance_topic_context)\n",
    "#         indices = torch.stack([argmax // distance_topic_context.shape[1], argmax % distance_topic_context.shape[1]], -1)\n",
    "#         triplet_sentence_only.append(sentences[indices[0]])\n",
    "#     test_triplets_sentence_only.append(tuple(triplet_sentence_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d8c17302-edd2-4022-aea8-f54f4e73a9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "if os.path.exists(\"data/data_new_format/triplets/train_triplets_sentence_only.pkl\"):\n",
    "    with open(\"data/data_new_format/triplets/train_triplets_sentence_only.pkl\", \"rb\") as file:\n",
    "        train_triplets_sentence_only = pickle.load(file)\n",
    "else:\n",
    "    print(\"train triplets not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8bd7e6e5-3e9e-4dab-a19d-73a152f4c6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train triplets not loaded\n",
      "test triplets not loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"data/data_new_format/triplets/test_triplets_sentence_only.pkl\"):\n",
    "    with open(\"data/data_new_format/triplets/test_triplets_sentence_only.pkl\", \"rb\") as file:\n",
    "        test_triplets_sentence_only = pickle.load(file)\n",
    "else:\n",
    "    print(\"test triplets not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ddfda122-b176-4f68-b5fd-bb838fd6ed9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TripletsDataset(Dataset):\n",
    "    \"\"\"Triplets Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, triplets, transform=None):\n",
    "        self.triplets = triplets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.triplets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "train_triplets_dataset = TripletsDataset(train_triplets)\n",
    "test_triplets_dataset = TripletsDataset(test_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ed0f4-5838-4bb2-9664-54eea7c8e46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823dd229-fa8b-442f-bb69-4b9057fde295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if single_match:\n",
    "    huggingface_model_name = 'allenai/aspire-contextualsentence-singlem-biomed' # single match\n",
    "else:\n",
    "    huggingface_model_name = 'allenai/aspire-contextualsentence-multim-biomed'  # multi match\n",
    "    ot_distance = AllPairMaskedWasserstein({}, device)\n",
    "aspire_tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, cache_dir=\"/cs/labs/tomhope/taltatal/cache\")\n",
    "aspire_mv_model = AspireConSent(huggingface_model_name, device).to(device)\n",
    "# Empty dict of hyper params will force class to use defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ef1ee4-b4ad-46f1-b8f7-a606f7ae14d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_model(docs, tokenizer, model, device=torch.device(\"cpu\")):\n",
    "    bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=docs,\n",
    "                                                              pt_lm_tokenizer=tokenizer)\n",
    "    # move batch to device, bert_batch is a dict\n",
    "    for k, v in bert_batch.items():\n",
    "        bert_batch[k] = v.to(device) if type(v) == torch.Tensor else v\n",
    "    # abs_lens is a list\n",
    "    abs_lens = torch.tensor(abs_lens, dtype=torch.long, device=device)\n",
    "\n",
    "    clsreps, contextual_sent_reps = model.forward(bert_batch=bert_batch,\n",
    "                                                  abs_lens=abs_lens,\n",
    "                                                  sent_tok_idxs=sent_token_idxs)\n",
    "    return abs_lens, contextual_sent_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f890f310-be76-4b24-b159-52d59031e2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_to_same_size_along_axis(tensor_list, axis=0):\n",
    "    max_size = max([x.shape[axis] for x in tensor_list])\n",
    "    padded_list = []\n",
    "    for tensor in tensor_list:\n",
    "        if tensor.shape[axis] < max_size:\n",
    "            pad_size = max_size - tensor.shape[axis]\n",
    "            padded_list.append(F.pad(input=tensor, pad=(0, 0, 0, pad_size, 0, 0), mode='constant', value=0))\n",
    "        else:\n",
    "            padded_list.append(tensor)\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c3e1b-db83-49b1-a0d1-3e05ee31c7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "Loss = torch.nn.TripletMarginLoss(margin=1)\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 1\n",
    "\n",
    "train_triplets_loader = DataLoader(train_triplets_dataset, batch_size=1, shuffle=True)\n",
    "test_triplets_loader = DataLoader(test_triplets_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(aspire_mv_model.parameters(), lr=learning_rate)\n",
    "# Fine-Tuning\n",
    "aspire_mv_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dda01d94-7251-41d3-8249-7ebdda8f70de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Tuning Epoch 1/1, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm.tqdm(train_triplets_loader):\n",
    "#         break\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device, dtype=torch.float32)\n",
    "\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         outputs_probabilities = torch.sigmoid(outputs.logits.view(-1))\n",
    "#         loss = Loss(outputs_probabilities, target=labels)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     avg_loss = total_loss / len(train_triplets_loader)\n",
    "\n",
    "#     print(f\"Fine Tuning Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
